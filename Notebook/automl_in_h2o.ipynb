{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "# Gradient Boosted Trees and AutoML\n",
    "\n",
    "Last updated: Aug 05th 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "This Gradient Notebook is part of the project *Gradient Boosted Trees and AutoML* at https://github.com/gradient-ai/Gradient-Boosted-Trees-and-AutoML .\n",
    "\n",
    "Business and other problems not amenable to deep learning are often best solved by using well-tuned Gradient-boosted decision trees. These methods are, like deep learning, capable of solving arbitrarily complex problems via nonlinear mappings, but can do so without requiring the large training sets and compute-intensive processing that deep learning sometimes can.\n",
    "\n",
    "This project shows that such methods are supported on Gradient by demonstrating training of **gradient-boosted decision trees** (GBT) using the well-known open source machine learning (ML) library H2O.\n",
    "\n",
    "We also show H2O's **automated machine learning** (AutoML) capability that can search the model hyperparameter tuning space. This can both save the user time required to so do manually, and produce better results by finding hyperparameter combinations that the user may miss. AutoML used in this way can surpass even expert human data scientists in some situations.\n",
    "\n",
    "H2O's AutoML includes within it another well-known GBT library, **XGBoost**.\n",
    "\n",
    "This project does not aim to show extensive model tuning, large datasets, or specific business problems, but to show the **end-to-end** combination of data preparation, model training, and deployment to production of the H2O model that is enabled within Gradient. We therefore show the commonly used [Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income) from the UCI ML repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## Setup\n",
    "This Notebook runs on the Gradient container `tensorflow/tensorflow:2.4.1-gpu-jupyter`, and requires the installation of H2O, and hence Java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h2o==3.32.1.3\n",
      "  Downloading h2o-3.32.1.3.tar.gz (164.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 164.8 MB 113 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from h2o==3.32.1.3) (2.25.1)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 36.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colorama>=0.3.8\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->h2o==3.32.1.3) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->h2o==3.32.1.3) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->h2o==3.32.1.3) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->h2o==3.32.1.3) (2.6)\n",
      "Building wheels for collected packages: h2o, future\n",
      "  Building wheel for h2o (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for h2o: filename=h2o-3.32.1.3-py2.py3-none-any.whl size=164854343 sha256=1540fb7695e05071c9f9bd487c737368bf50fc7480f4dc5d9b6afd5d43f7b8c1\n",
      "  Stored in directory: /root/.cache/pip/wheels/72/00/18/d1ed0b56eb5efd5e96b48828c07bd131ff8829a6d16fcef39d\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=c79b75cd6b51769fb48f5b00c8c50989e6b8a0a1c22da05b1a9667f9fe645000\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
      "Successfully built h2o future\n",
      "Installing collected packages: tabulate, future, colorama, h2o\n",
      "Successfully installed colorama-0.4.4 future-0.18.2 h2o-3.32.1.3 tabulate-0.8.9\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install H2O\n",
    "!pip install h2o==3.32.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting install-jdk==0.3.0\n",
      "  Downloading install-jdk-0.3.0.tar.gz (3.8 kB)\n",
      "Building wheels for collected packages: install-jdk\n",
      "  Building wheel for install-jdk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for install-jdk: filename=install_jdk-0.3.0-py3-none-any.whl size=3739 sha256=1f8bc560917a8cd6f620d7aa85b2914ebbf713ff3bff69a081d060d55ca62d6a\n",
      "  Stored in directory: /root/.cache/pip/wheels/3a/5f/ee/3ff795a99fbd5222097c94dff4535ea4b5c2a91a234daa6611\n",
      "Successfully built install-jdk\n",
      "Installing collected packages: install-jdk\n",
      "Successfully installed install-jdk-0.3.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Java using https://pypi.org/project/install-jdk/\n",
    "!pip install install-jdk==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.jre/jdk-11.0.11+9-jre'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This may show an error if jdk is already installed from a previous run of the notebook,\n",
    "# but it is OK to proceed\n",
    "\n",
    "import jdk\n",
    "jdk.install('11', jre=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Add the Java to the path so that H2O can see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='echo $PATH', returncode=0, stdout='/root/.jre/jdk-11.0.11+9-jre/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\n')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "os.environ['PATH'] = \"/root/.jre/jdk-11.0.11+9-jre/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n",
    "subprocess.run('echo $PATH', shell=True, check=True, stdout=subprocess.PIPE, universal_newlines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "H2O runs as a server, so we start this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"11.0.11\" 2021-04-20; OpenJDK Runtime Environment AdoptOpenJDK-11.0.11+9 (build 11.0.11+9); OpenJDK 64-Bit Server VM AdoptOpenJDK-11.0.11+9 (build 11.0.11+9, mixed mode)\n",
      "  Starting server from /usr/local/lib/python3.6/dist-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpq5ho0wi_\n",
      "  JVM stdout: /tmp/tmpq5ho0wi_/h2o_unknownUser_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpq5ho0wi_/h2o_unknownUser_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Etc/GMT</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.32.1.3</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>2 months and 1 day </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_unknownUser_rzhigf</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>6.750 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>11</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>11</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.6.9 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         02 secs\n",
       "H2O_cluster_timezone:       Etc/GMT\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.32.1.3\n",
       "H2O_cluster_version_age:    2 months and 1 day\n",
       "H2O_cluster_name:           H2O_from_python_unknownUser_rzhigf\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    6.750 Gb\n",
       "H2O_cluster_total_cores:    11\n",
       "H2O_cluster_allowed_cores:  11\n",
       "H2O_cluster_status:         accepting new members, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.6.9 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## Prepare data\n",
    "We load the slightly modified version of the income dataset supplied with the repo. This saves some data cleaning lines not relevant to this project such as removing the final empty line.\n",
    "\n",
    "The original data is at the [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/census+income) .\n",
    "\n",
    "H2O provides an `import_file` method that enables convenient import of a CSV file to a dataframe. This process is fine here because the data are small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "df = h2o.import_file(path = \"../income.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "The data can be viewed. It consists of 14 columns of demographic information of mixed data type, and a binary ground-truth column `yearly-income`.\n",
    "\n",
    "Our task is to build a binary supervised ML classification model to predict whether a person's income is low (`<=50K`) or high (`>50K`).\n",
    "\n",
    "This has obvious potential business applications, such as deciding who to market cheap or expensive products to, but we will not explore those here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  age</th><th>workclass       </th><th style=\"text-align: right;\">  fnlwgt</th><th>education  </th><th style=\"text-align: right;\">  education-num</th><th>marital-status       </th><th>occupation       </th><th>relationship  </th><th>race  </th><th>sex   </th><th style=\"text-align: right;\">  capital-gain</th><th style=\"text-align: right;\">  capital-loss</th><th style=\"text-align: right;\">  hours-per-week</th><th>native-country  </th><th>yearly-income  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">   39</td><td>State-gov       </td><td style=\"text-align: right;\">   77516</td><td>Bachelors  </td><td style=\"text-align: right;\">             13</td><td>Never-married        </td><td>Adm-clerical     </td><td>Not-in-family </td><td>White </td><td>Male  </td><td style=\"text-align: right;\">          2174</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">              40</td><td>United-States   </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   50</td><td>Self-emp-not-inc</td><td style=\"text-align: right;\">   83311</td><td>Bachelors  </td><td style=\"text-align: right;\">             13</td><td>Married-civ-spouse   </td><td>Exec-managerial  </td><td>Husband       </td><td>White </td><td>Male  </td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">              13</td><td>United-States   </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   38</td><td>Private         </td><td style=\"text-align: right;\">  215646</td><td>HS-grad    </td><td style=\"text-align: right;\">              9</td><td>Divorced             </td><td>Handlers-cleaners</td><td>Not-in-family </td><td>White </td><td>Male  </td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">              40</td><td>United-States   </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   53</td><td>Private         </td><td style=\"text-align: right;\">  234721</td><td>11th       </td><td style=\"text-align: right;\">              7</td><td>Married-civ-spouse   </td><td>Handlers-cleaners</td><td>Husband       </td><td>Black </td><td>Male  </td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">              40</td><td>United-States   </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   28</td><td>Private         </td><td style=\"text-align: right;\">  338409</td><td>Bachelors  </td><td style=\"text-align: right;\">             13</td><td>Married-civ-spouse   </td><td>Prof-specialty   </td><td>Wife          </td><td>Black </td><td>Female</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">              40</td><td>Cuba            </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   37</td><td>Private         </td><td style=\"text-align: right;\">  284582</td><td>Masters    </td><td style=\"text-align: right;\">             14</td><td>Married-civ-spouse   </td><td>Exec-managerial  </td><td>Wife          </td><td>White </td><td>Female</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">              40</td><td>United-States   </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   49</td><td>Private         </td><td style=\"text-align: right;\">  160187</td><td>9th        </td><td style=\"text-align: right;\">              5</td><td>Married-spouse-absent</td><td>Other-service    </td><td>Not-in-family </td><td>Black </td><td>Female</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">              16</td><td>Jamaica         </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   52</td><td>Self-emp-not-inc</td><td style=\"text-align: right;\">  209642</td><td>HS-grad    </td><td style=\"text-align: right;\">              9</td><td>Married-civ-spouse   </td><td>Exec-managerial  </td><td>Husband       </td><td>White </td><td>Male  </td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">              45</td><td>United-States   </td><td>&gt;50K           </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   31</td><td>Private         </td><td style=\"text-align: right;\">   45781</td><td>Masters    </td><td style=\"text-align: right;\">             14</td><td>Never-married        </td><td>Prof-specialty   </td><td>Not-in-family </td><td>White </td><td>Female</td><td style=\"text-align: right;\">         14084</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">              50</td><td>United-States   </td><td>&gt;50K           </td></tr>\n",
       "<tr><td style=\"text-align: right;\">   42</td><td>Private         </td><td style=\"text-align: right;\">  159449</td><td>Bachelors  </td><td style=\"text-align: right;\">             13</td><td>Married-civ-spouse   </td><td>Exec-managerial  </td><td>Husband       </td><td>White </td><td>Male  </td><td style=\"text-align: right;\">          5178</td><td style=\"text-align: right;\">             0</td><td style=\"text-align: right;\">              40</td><td>United-States   </td><td>&gt;50K           </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "We can also summarize the dataframe with various statistics particularly useful for the exploratory data science that we are performing, using H2O's `summary()` method. Information includes min/max/spread, but also data type, number of zeros, and number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>age               </th><th>workclass       </th><th>fnlwgt            </th><th>education  </th><th>education-num     </th><th>marital-status       </th><th>occupation       </th><th>relationship  </th><th>race  </th><th>sex   </th><th>capital-gain      </th><th>capital-loss      </th><th>hours-per-week    </th><th>native-country  </th><th>yearly-income  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>int               </td><td>enum            </td><td>int               </td><td>enum       </td><td>int               </td><td>enum                 </td><td>enum             </td><td>enum          </td><td>enum  </td><td>enum  </td><td>int               </td><td>int               </td><td>int               </td><td>enum            </td><td>enum           </td></tr>\n",
       "<tr><td>mins   </td><td>17.0              </td><td>                </td><td>12285.0           </td><td>           </td><td>1.0               </td><td>                     </td><td>                 </td><td>              </td><td>      </td><td>      </td><td>0.0               </td><td>0.0               </td><td>1.0               </td><td>                </td><td>               </td></tr>\n",
       "<tr><td>mean   </td><td>38.581646755320776</td><td>                </td><td>189778.36651208502</td><td>           </td><td>10.080679340315099</td><td>                     </td><td>                 </td><td>              </td><td>      </td><td>      </td><td>1077.6488437087312</td><td>87.303829734959   </td><td>40.437455852092995</td><td>                </td><td>               </td></tr>\n",
       "<tr><td>maxs   </td><td>90.0              </td><td>                </td><td>1484705.0         </td><td>           </td><td>16.0              </td><td>                     </td><td>                 </td><td>              </td><td>      </td><td>      </td><td>99999.0           </td><td>4356.0            </td><td>99.0              </td><td>                </td><td>               </td></tr>\n",
       "<tr><td>sigma  </td><td>13.64043255358134 </td><td>                </td><td>105549.97769702224</td><td>           </td><td>2.5727203320673877</td><td>                     </td><td>                 </td><td>              </td><td>      </td><td>      </td><td>7385.29208484034  </td><td>402.96021864899967</td><td>12.347428681731843</td><td>                </td><td>               </td></tr>\n",
       "<tr><td>zeros  </td><td>0                 </td><td>                </td><td>0                 </td><td>           </td><td>0                 </td><td>                     </td><td>                 </td><td>              </td><td>      </td><td>      </td><td>29849             </td><td>31042             </td><td>0                 </td><td>                </td><td>               </td></tr>\n",
       "<tr><td>missing</td><td>0                 </td><td>0               </td><td>0                 </td><td>0          </td><td>0                 </td><td>0                    </td><td>0                </td><td>0             </td><td>0     </td><td>0     </td><td>0                 </td><td>0                 </td><td>0                 </td><td>0               </td><td>0              </td></tr>\n",
       "<tr><td>0      </td><td>39.0              </td><td>State-gov       </td><td>77516.0           </td><td>Bachelors  </td><td>13.0              </td><td>Never-married        </td><td>Adm-clerical     </td><td>Not-in-family </td><td>White </td><td>Male  </td><td>2174.0            </td><td>0.0               </td><td>40.0              </td><td>United-States   </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td>1      </td><td>50.0              </td><td>Self-emp-not-inc</td><td>83311.0           </td><td>Bachelors  </td><td>13.0              </td><td>Married-civ-spouse   </td><td>Exec-managerial  </td><td>Husband       </td><td>White </td><td>Male  </td><td>0.0               </td><td>0.0               </td><td>13.0              </td><td>United-States   </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td>2      </td><td>38.0              </td><td>Private         </td><td>215646.0          </td><td>HS-grad    </td><td>9.0               </td><td>Divorced             </td><td>Handlers-cleaners</td><td>Not-in-family </td><td>White </td><td>Male  </td><td>0.0               </td><td>0.0               </td><td>40.0              </td><td>United-States   </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td>3      </td><td>53.0              </td><td>Private         </td><td>234721.0          </td><td>11th       </td><td>7.0               </td><td>Married-civ-spouse   </td><td>Handlers-cleaners</td><td>Husband       </td><td>Black </td><td>Male  </td><td>0.0               </td><td>0.0               </td><td>40.0              </td><td>United-States   </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td>4      </td><td>28.0              </td><td>Private         </td><td>338409.0          </td><td>Bachelors  </td><td>13.0              </td><td>Married-civ-spouse   </td><td>Prof-specialty   </td><td>Wife          </td><td>Black </td><td>Female</td><td>0.0               </td><td>0.0               </td><td>40.0              </td><td>Cuba            </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td>5      </td><td>37.0              </td><td>Private         </td><td>284582.0          </td><td>Masters    </td><td>14.0              </td><td>Married-civ-spouse   </td><td>Exec-managerial  </td><td>Wife          </td><td>White </td><td>Female</td><td>0.0               </td><td>0.0               </td><td>40.0              </td><td>United-States   </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td>6      </td><td>49.0              </td><td>Private         </td><td>160187.0          </td><td>9th        </td><td>5.0               </td><td>Married-spouse-absent</td><td>Other-service    </td><td>Not-in-family </td><td>Black </td><td>Female</td><td>0.0               </td><td>0.0               </td><td>16.0              </td><td>Jamaica         </td><td>&lt;=50K          </td></tr>\n",
       "<tr><td>7      </td><td>52.0              </td><td>Self-emp-not-inc</td><td>209642.0          </td><td>HS-grad    </td><td>9.0               </td><td>Married-civ-spouse   </td><td>Exec-managerial  </td><td>Husband       </td><td>White </td><td>Male  </td><td>0.0               </td><td>0.0               </td><td>45.0              </td><td>United-States   </td><td>&gt;50K           </td></tr>\n",
       "<tr><td>8      </td><td>31.0              </td><td>Private         </td><td>45781.0           </td><td>Masters    </td><td>14.0              </td><td>Never-married        </td><td>Prof-specialty   </td><td>Not-in-family </td><td>White </td><td>Female</td><td>14084.0           </td><td>0.0               </td><td>50.0              </td><td>United-States   </td><td>&gt;50K           </td></tr>\n",
       "<tr><td>9      </td><td>42.0              </td><td>Private         </td><td>159449.0          </td><td>Bachelors  </td><td>13.0              </td><td>Married-civ-spouse   </td><td>Exec-managerial  </td><td>Husband       </td><td>White </td><td>Male  </td><td>5178.0            </td><td>0.0               </td><td>40.0              </td><td>United-States   </td><td>&gt;50K           </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "We separate the data feature columns (1-14) from the label in column 15 (yearly-income)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']\n"
     ]
    }
   ],
   "source": [
    "# Feature columns and label\n",
    "y = \"yearly-income\"\n",
    "x = df.columns\n",
    "del x[14]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "And split the data into a training, validation, and testing set.\n",
    "\n",
    "In H2O, the datasets are put into their *hex* format, which improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "train, valid, test = df.split_frame(\n",
    "    ratios = [0.6,0.2],\n",
    "    seed = 123456,\n",
    "    destination_frames=['train.hex','valid.hex','test.hex']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## Train the model using AutoML\n",
    "\n",
    "Model training can then be performed using AutoML. Here we set the maximum number of models to search to be 20. The training takes a few minutes to run, which can be measure by `%%time` as the first command in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n",
      "CPU times: user 12.9 s, sys: 416 ms, total: 13.3 s\n",
      "Wall time: 9min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Run AutoML\n",
    "aml = H2OAutoML(max_models=20, seed=1)\n",
    "aml.train(x=x, y=y, training_frame=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "We see from the searched models that a variety of configurations have been tried, including:\n",
    "\n",
    " - Regular GBT (aka. GBM, gradient boosting machine)\n",
    " - XGBoost model with grid of hyperparameter values\n",
    " - A deep learning model\n",
    " - Random forest\n",
    " - Stacked ensembles of models (stacking = feed model output into next model input)\n",
    "\n",
    "For full details of the models searched in AutoML, see [H2O's AutoML documentation](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html).\n",
    "\n",
    "We also see in the table various metrics for the model performance on the validation set, the leaderboard here being ordered by `auc`, which is the area under curve of model true versus false positive rate. Other [metrics](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/performance-and-prediction.html?#classification) shown include logarithmic loss, area under precision-recall curve, and mean squared error.\n",
    "\n",
    "Gradient includes support for [tracking model metrics](https://docs.paperspace.com/gradient/data/metrics-overview), both in model experimentation and production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                           </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">   aucpr</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">      mse</th><th style=\"text-align: right;\">  training_time_ms</th><th style=\"text-align: right;\">  predict_time_per_row_ms</th><th>algo           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_AllModels_AutoML_20210721_040517   </td><td style=\"text-align: right;\">0.927516</td><td style=\"text-align: right;\"> 0.280334</td><td style=\"text-align: right;\">0.828239</td><td style=\"text-align: right;\">              0.16976 </td><td style=\"text-align: right;\">0.298357</td><td style=\"text-align: right;\">0.0890171</td><td style=\"text-align: right;\">              2791</td><td style=\"text-align: right;\">                 0.043219</td><td>StackedEnsemble</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_AutoML_20210721_040517</td><td style=\"text-align: right;\">0.926968</td><td style=\"text-align: right;\"> 0.281188</td><td style=\"text-align: right;\">0.827822</td><td style=\"text-align: right;\">              0.175579</td><td style=\"text-align: right;\">0.298627</td><td style=\"text-align: right;\">0.0891782</td><td style=\"text-align: right;\">              1563</td><td style=\"text-align: right;\">                 0.017421</td><td>StackedEnsemble</td></tr>\n",
       "<tr><td>XGBoost_grid__1_AutoML_20210721_040517_model_4     </td><td style=\"text-align: right;\">0.925388</td><td style=\"text-align: right;\"> 0.283678</td><td style=\"text-align: right;\">0.824562</td><td style=\"text-align: right;\">              0.176857</td><td style=\"text-align: right;\">0.300212</td><td style=\"text-align: right;\">0.0901273</td><td style=\"text-align: right;\">              1132</td><td style=\"text-align: right;\">                 0.003136</td><td>XGBoost        </td></tr>\n",
       "<tr><td>GBM_1_AutoML_20210721_040517                       </td><td style=\"text-align: right;\">0.925145</td><td style=\"text-align: right;\"> 0.285528</td><td style=\"text-align: right;\">0.823719</td><td style=\"text-align: right;\">              0.172024</td><td style=\"text-align: right;\">0.30061 </td><td style=\"text-align: right;\">0.0903661</td><td style=\"text-align: right;\">              1852</td><td style=\"text-align: right;\">                 0.013197</td><td>GBM            </td></tr>\n",
       "<tr><td>XGBoost_3_AutoML_20210721_040517                   </td><td style=\"text-align: right;\">0.925114</td><td style=\"text-align: right;\"> 0.284771</td><td style=\"text-align: right;\">0.822262</td><td style=\"text-align: right;\">              0.16712 </td><td style=\"text-align: right;\">0.301022</td><td style=\"text-align: right;\">0.0906144</td><td style=\"text-align: right;\">              1428</td><td style=\"text-align: right;\">                 0.00234 </td><td>XGBoost        </td></tr>\n",
       "<tr><td>GBM_2_AutoML_20210721_040517                       </td><td style=\"text-align: right;\">0.924923</td><td style=\"text-align: right;\"> 0.286024</td><td style=\"text-align: right;\">0.823077</td><td style=\"text-align: right;\">              0.173164</td><td style=\"text-align: right;\">0.300807</td><td style=\"text-align: right;\">0.0904848</td><td style=\"text-align: right;\">              1602</td><td style=\"text-align: right;\">                 0.01191 </td><td>GBM            </td></tr>\n",
       "<tr><td>GBM_3_AutoML_20210721_040517                       </td><td style=\"text-align: right;\">0.92415 </td><td style=\"text-align: right;\"> 0.287323</td><td style=\"text-align: right;\">0.821873</td><td style=\"text-align: right;\">              0.173311</td><td style=\"text-align: right;\">0.301484</td><td style=\"text-align: right;\">0.0908926</td><td style=\"text-align: right;\">              1586</td><td style=\"text-align: right;\">                 0.011957</td><td>GBM            </td></tr>\n",
       "<tr><td>GBM_grid__1_AutoML_20210721_040517_model_1         </td><td style=\"text-align: right;\">0.922166</td><td style=\"text-align: right;\"> 0.291503</td><td style=\"text-align: right;\">0.816787</td><td style=\"text-align: right;\">              0.179959</td><td style=\"text-align: right;\">0.303526</td><td style=\"text-align: right;\">0.092128 </td><td style=\"text-align: right;\">              1373</td><td style=\"text-align: right;\">                 0.012912</td><td>GBM            </td></tr>\n",
       "<tr><td>XGBoost_grid__1_AutoML_20210721_040517_model_3     </td><td style=\"text-align: right;\">0.922007</td><td style=\"text-align: right;\"> 0.290389</td><td style=\"text-align: right;\">0.816119</td><td style=\"text-align: right;\">              0.168792</td><td style=\"text-align: right;\">0.303951</td><td style=\"text-align: right;\">0.0923859</td><td style=\"text-align: right;\">              1641</td><td style=\"text-align: right;\">                 0.002229</td><td>XGBoost        </td></tr>\n",
       "<tr><td>GBM_4_AutoML_20210721_040517                       </td><td style=\"text-align: right;\">0.921542</td><td style=\"text-align: right;\"> 0.292358</td><td style=\"text-align: right;\">0.816709</td><td style=\"text-align: right;\">              0.167862</td><td style=\"text-align: right;\">0.304239</td><td style=\"text-align: right;\">0.0925616</td><td style=\"text-align: right;\">              1693</td><td style=\"text-align: right;\">                 0.010229</td><td>GBM            </td></tr>\n",
       "<tr><td>XGBoost_grid__1_AutoML_20210721_040517_model_1     </td><td style=\"text-align: right;\">0.920124</td><td style=\"text-align: right;\"> 0.293777</td><td style=\"text-align: right;\">0.810726</td><td style=\"text-align: right;\">              0.174024</td><td style=\"text-align: right;\">0.306053</td><td style=\"text-align: right;\">0.0936684</td><td style=\"text-align: right;\">              1105</td><td style=\"text-align: right;\">                 0.002016</td><td>XGBoost        </td></tr>\n",
       "<tr><td>GBM_5_AutoML_20210721_040517                       </td><td style=\"text-align: right;\">0.919805</td><td style=\"text-align: right;\"> 0.295047</td><td style=\"text-align: right;\">0.809737</td><td style=\"text-align: right;\">              0.176791</td><td style=\"text-align: right;\">0.306613</td><td style=\"text-align: right;\">0.0940116</td><td style=\"text-align: right;\">              1825</td><td style=\"text-align: right;\">                 0.012591</td><td>GBM            </td></tr>\n",
       "<tr><td>GBM_grid__1_AutoML_20210721_040517_model_2         </td><td style=\"text-align: right;\">0.919622</td><td style=\"text-align: right;\"> 0.296454</td><td style=\"text-align: right;\">0.808166</td><td style=\"text-align: right;\">              0.169346</td><td style=\"text-align: right;\">0.307024</td><td style=\"text-align: right;\">0.0942634</td><td style=\"text-align: right;\">              2430</td><td style=\"text-align: right;\">                 0.014451</td><td>GBM            </td></tr>\n",
       "<tr><td>XGBoost_1_AutoML_20210721_040517                   </td><td style=\"text-align: right;\">0.917721</td><td style=\"text-align: right;\"> 0.29918 </td><td style=\"text-align: right;\">0.807836</td><td style=\"text-align: right;\">              0.182311</td><td style=\"text-align: right;\">0.308987</td><td style=\"text-align: right;\">0.0954732</td><td style=\"text-align: right;\">              1633</td><td style=\"text-align: right;\">                 0.002271</td><td>XGBoost        </td></tr>\n",
       "<tr><td>XGBoost_grid__1_AutoML_20210721_040517_model_2     </td><td style=\"text-align: right;\">0.914106</td><td style=\"text-align: right;\"> 0.30665 </td><td style=\"text-align: right;\">0.799365</td><td style=\"text-align: right;\">              0.17817 </td><td style=\"text-align: right;\">0.312989</td><td style=\"text-align: right;\">0.0979622</td><td style=\"text-align: right;\">              1195</td><td style=\"text-align: right;\">                 0.002329</td><td>XGBoost        </td></tr>\n",
       "<tr><td>XGBoost_2_AutoML_20210721_040517                   </td><td style=\"text-align: right;\">0.912909</td><td style=\"text-align: right;\"> 0.309072</td><td style=\"text-align: right;\">0.796666</td><td style=\"text-align: right;\">              0.182337</td><td style=\"text-align: right;\">0.314178</td><td style=\"text-align: right;\">0.0987075</td><td style=\"text-align: right;\">              2338</td><td style=\"text-align: right;\">                 0.002253</td><td>XGBoost        </td></tr>\n",
       "<tr><td>DRF_1_AutoML_20210721_040517                       </td><td style=\"text-align: right;\">0.910646</td><td style=\"text-align: right;\"> 0.342976</td><td style=\"text-align: right;\">0.796839</td><td style=\"text-align: right;\">              0.191282</td><td style=\"text-align: right;\">0.312544</td><td style=\"text-align: right;\">0.0976838</td><td style=\"text-align: right;\">              2409</td><td style=\"text-align: right;\">                 0.009805</td><td>DRF            </td></tr>\n",
       "<tr><td>XRT_1_AutoML_20210721_040517                       </td><td style=\"text-align: right;\">0.910375</td><td style=\"text-align: right;\"> 0.311692</td><td style=\"text-align: right;\">0.797673</td><td style=\"text-align: right;\">              0.191194</td><td style=\"text-align: right;\">0.313144</td><td style=\"text-align: right;\">0.098059 </td><td style=\"text-align: right;\">              2003</td><td style=\"text-align: right;\">                 0.010056</td><td>DRF            </td></tr>\n",
       "<tr><td>DeepLearning_1_AutoML_20210721_040517              </td><td style=\"text-align: right;\">0.908007</td><td style=\"text-align: right;\"> 0.317593</td><td style=\"text-align: right;\">0.773418</td><td style=\"text-align: right;\">              0.183058</td><td style=\"text-align: right;\">0.318782</td><td style=\"text-align: right;\">0.101622 </td><td style=\"text-align: right;\">              1327</td><td style=\"text-align: right;\">                 0.002407</td><td>DeepLearning   </td></tr>\n",
       "<tr><td>GLM_1_AutoML_20210721_040517                       </td><td style=\"text-align: right;\">0.90694 </td><td style=\"text-align: right;\"> 0.318976</td><td style=\"text-align: right;\">0.770627</td><td style=\"text-align: right;\">              0.185024</td><td style=\"text-align: right;\">0.319614</td><td style=\"text-align: right;\">0.102153 </td><td style=\"text-align: right;\">              1700</td><td style=\"text-align: right;\">                 0.001705</td><td>GLM            </td></tr>\n",
       "<tr><td>DeepLearning_grid__1_AutoML_20210721_040517_model_1</td><td style=\"text-align: right;\">0.906031</td><td style=\"text-align: right;\"> 0.342893</td><td style=\"text-align: right;\">0.775935</td><td style=\"text-align: right;\">              0.180782</td><td style=\"text-align: right;\">0.319573</td><td style=\"text-align: right;\">0.102127 </td><td style=\"text-align: right;\">             42994</td><td style=\"text-align: right;\">                 0.003064</td><td>DeepLearning   </td></tr>\n",
       "<tr><td>DeepLearning_grid__2_AutoML_20210721_040517_model_1</td><td style=\"text-align: right;\">0.903074</td><td style=\"text-align: right;\"> 0.341221</td><td style=\"text-align: right;\">0.763071</td><td style=\"text-align: right;\">              0.189912</td><td style=\"text-align: right;\">0.322948</td><td style=\"text-align: right;\">0.104295 </td><td style=\"text-align: right;\">             57236</td><td style=\"text-align: right;\">                 0.003667</td><td>DeepLearning   </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = h2o.automl.get_leaderboard(aml, extra_columns = 'ALL')\n",
    "lb.head(rows=lb.nrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "The best model is the stacked ensemble, and we can see its properties in more detail. These include further metrics on model performance, such as the F-score harmonic mean of precision and recall, and the confusion matrix between predicted and ground truth labels, showing true and false positives and negatives. The information is shown for the training data, and then for the (cross-validated) validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_AutoML_20210721_040517\n",
      "\n",
      "No model summary for this model\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.0741088222473043\n",
      "RMSE: 0.2722293559616676\n",
      "LogLoss: 0.23663590818870123\n",
      "Null degrees of freedom: 10046\n",
      "Residual degrees of freedom: 10040\n",
      "Null deviance: 11134.613562297909\n",
      "Residual deviance: 4754.961939143763\n",
      "AIC: 4768.961939143763\n",
      "AUC: 0.9520188734229607\n",
      "AUCPR: 0.8804096756815729\n",
      "Gini: 0.9040377468459213\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.42508909310298126: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b><=50K</b></td>\n",
       "<td><b>>50K</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td><=50K</td>\n",
       "<td>7095.0</td>\n",
       "<td>514.0</td>\n",
       "<td>0.0676</td>\n",
       "<td> (514.0/7609.0)</td></tr>\n",
       "<tr><td>>50K</td>\n",
       "<td>539.0</td>\n",
       "<td>1899.0</td>\n",
       "<td>0.2211</td>\n",
       "<td> (539.0/2438.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>7634.0</td>\n",
       "<td>2413.0</td>\n",
       "<td>0.1048</td>\n",
       "<td> (1053.0/10047.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       <=50K    >50K    Error    Rate\n",
       "-----  -------  ------  -------  ----------------\n",
       "<=50K  7095     514     0.0676   (514.0/7609.0)\n",
       ">50K   539      1899    0.2211   (539.0/2438.0)\n",
       "Total  7634     2413    0.1048   (1053.0/10047.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4250891</td>\n",
       "<td>0.7829314</td>\n",
       "<td>188.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2373109</td>\n",
       "<td>0.8465888</td>\n",
       "<td>263.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5626342</td>\n",
       "<td>0.8147846</td>\n",
       "<td>138.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4800965</td>\n",
       "<td>0.8970837</td>\n",
       "<td>166.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9983993</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0066038</td>\n",
       "<td>1.0</td>\n",
       "<td>390.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9983993</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4475590</td>\n",
       "<td>0.7148985</td>\n",
       "<td>179.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.3109446</td>\n",
       "<td>0.8742279</td>\n",
       "<td>233.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.2373109</td>\n",
       "<td>0.8773668</td>\n",
       "<td>263.0</td></tr>\n",
       "<tr><td>max tns</td>\n",
       "<td>0.9983993</td>\n",
       "<td>7609.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fns</td>\n",
       "<td>0.9983993</td>\n",
       "<td>2379.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fps</td>\n",
       "<td>0.0005743</td>\n",
       "<td>7609.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tps</td>\n",
       "<td>0.0066038</td>\n",
       "<td>2438.0</td>\n",
       "<td>390.0</td></tr>\n",
       "<tr><td>max tnr</td>\n",
       "<td>0.9983993</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fnr</td>\n",
       "<td>0.9983993</td>\n",
       "<td>0.9757998</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fpr</td>\n",
       "<td>0.0005743</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tpr</td>\n",
       "<td>0.0066038</td>\n",
       "<td>1.0</td>\n",
       "<td>390.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.425089     0.782931  188\n",
       "max f2                       0.237311     0.846589  263\n",
       "max f0point5                 0.562634     0.814785  138\n",
       "max accuracy                 0.480096     0.897084  166\n",
       "max precision                0.998399     1         0\n",
       "max recall                   0.00660379   1         390\n",
       "max specificity              0.998399     1         0\n",
       "max absolute_mcc             0.447559     0.714898  179\n",
       "max min_per_class_accuracy   0.310945     0.874228  233\n",
       "max mean_per_class_accuracy  0.237311     0.877367  263\n",
       "max tns                      0.998399     7609      0\n",
       "max fns                      0.998399     2379      0\n",
       "max fps                      0.000574306  7609      399\n",
       "max tps                      0.00660379   2438      390\n",
       "max tnr                      0.998399     1         0\n",
       "max fnr                      0.998399     0.9758    0\n",
       "max fpr                      0.000574306  1         399\n",
       "max tpr                      0.00660379   1         390"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 24.27 %, avg score: 24.31 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td>\n",
       "<td><b>kolmogorov_smirnov</b></td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0100528</td>\n",
       "<td>0.9975005</td>\n",
       "<td>4.1210008</td>\n",
       "<td>4.1210008</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9981390</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9981390</td>\n",
       "<td>0.0414274</td>\n",
       "<td>0.0414274</td>\n",
       "<td>312.1000820</td>\n",
       "<td>312.1000820</td>\n",
       "<td>0.0414274</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.0200060</td>\n",
       "<td>0.9955189</td>\n",
       "<td>4.1210008</td>\n",
       "<td>4.1210008</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9965840</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9973654</td>\n",
       "<td>0.0410172</td>\n",
       "<td>0.0824446</td>\n",
       "<td>312.1000820</td>\n",
       "<td>312.1000820</td>\n",
       "<td>0.0824446</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>0.0300587</td>\n",
       "<td>0.9925287</td>\n",
       "<td>4.1210008</td>\n",
       "<td>4.1210008</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9941371</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9962857</td>\n",
       "<td>0.0414274</td>\n",
       "<td>0.1238720</td>\n",
       "<td>312.1000820</td>\n",
       "<td>312.1000820</td>\n",
       "<td>0.1238720</td></tr>\n",
       "<tr><td>4</td>\n",
       "<td>0.0400119</td>\n",
       "<td>0.9865375</td>\n",
       "<td>4.1210008</td>\n",
       "<td>4.1210008</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9901623</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9947625</td>\n",
       "<td>0.0410172</td>\n",
       "<td>0.1648893</td>\n",
       "<td>312.1000820</td>\n",
       "<td>312.1000820</td>\n",
       "<td>0.1648893</td></tr>\n",
       "<tr><td>5</td>\n",
       "<td>0.0500647</td>\n",
       "<td>0.9751929</td>\n",
       "<td>4.1210008</td>\n",
       "<td>4.1210008</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9810366</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9920064</td>\n",
       "<td>0.0414274</td>\n",
       "<td>0.2063167</td>\n",
       "<td>312.1000820</td>\n",
       "<td>312.1000820</td>\n",
       "<td>0.2063167</td></tr>\n",
       "<tr><td>6</td>\n",
       "<td>0.1000299</td>\n",
       "<td>0.7868145</td>\n",
       "<td>3.8993534</td>\n",
       "<td>4.0102874</td>\n",
       "<td>0.9462151</td>\n",
       "<td>0.8820496</td>\n",
       "<td>0.9731343</td>\n",
       "<td>0.9370827</td>\n",
       "<td>0.1948318</td>\n",
       "<td>0.4011485</td>\n",
       "<td>289.9353366</td>\n",
       "<td>301.0287365</td>\n",
       "<td>0.3976001</td></tr>\n",
       "<tr><td>7</td>\n",
       "<td>0.1499950</td>\n",
       "<td>0.6548474</td>\n",
       "<td>3.1769468</td>\n",
       "<td>3.7326915</td>\n",
       "<td>0.7709163</td>\n",
       "<td>0.7201090</td>\n",
       "<td>0.9057731</td>\n",
       "<td>0.8648061</td>\n",
       "<td>0.1587367</td>\n",
       "<td>0.5598852</td>\n",
       "<td>217.6946848</td>\n",
       "<td>273.2691519</td>\n",
       "<td>0.5412230</td></tr>\n",
       "<tr><td>8</td>\n",
       "<td>0.2000597</td>\n",
       "<td>0.5201886</td>\n",
       "<td>2.7446029</td>\n",
       "<td>3.4854236</td>\n",
       "<td>0.6660040</td>\n",
       "<td>0.5836761</td>\n",
       "<td>0.8457711</td>\n",
       "<td>0.7944537</td>\n",
       "<td>0.1374077</td>\n",
       "<td>0.6972929</td>\n",
       "<td>174.4602932</td>\n",
       "<td>248.5423579</td>\n",
       "<td>0.6565516</td></tr>\n",
       "<tr><td>9</td>\n",
       "<td>0.2999900</td>\n",
       "<td>0.3224050</td>\n",
       "<td>1.6869834</td>\n",
       "<td>2.8863413</td>\n",
       "<td>0.4093625</td>\n",
       "<td>0.4109287</td>\n",
       "<td>0.7003981</td>\n",
       "<td>0.6666969</td>\n",
       "<td>0.1685808</td>\n",
       "<td>0.8658737</td>\n",
       "<td>68.6983404</td>\n",
       "<td>188.6341318</td>\n",
       "<td>0.7471984</td></tr>\n",
       "<tr><td>10</td>\n",
       "<td>0.4000199</td>\n",
       "<td>0.1622398</td>\n",
       "<td>0.8529037</td>\n",
       "<td>2.3778554</td>\n",
       "<td>0.2069652</td>\n",
       "<td>0.2369063</td>\n",
       "<td>0.5770092</td>\n",
       "<td>0.5592225</td>\n",
       "<td>0.0853158</td>\n",
       "<td>0.9511895</td>\n",
       "<td>-14.7096348</td>\n",
       "<td>137.7855412</td>\n",
       "<td>0.7277699</td></tr>\n",
       "<tr><td>11</td>\n",
       "<td>0.5000498</td>\n",
       "<td>0.0708357</td>\n",
       "<td>0.3239394</td>\n",
       "<td>1.9669904</td>\n",
       "<td>0.0786070</td>\n",
       "<td>0.1117973</td>\n",
       "<td>0.4773089</td>\n",
       "<td>0.4697196</td>\n",
       "<td>0.0324036</td>\n",
       "<td>0.9835931</td>\n",
       "<td>-67.6060632</td>\n",
       "<td>96.6990439</td>\n",
       "<td>0.6384755</td></tr>\n",
       "<tr><td>12</td>\n",
       "<td>0.5999801</td>\n",
       "<td>0.0306384</td>\n",
       "<td>0.1190329</td>\n",
       "<td>1.6592019</td>\n",
       "<td>0.0288845</td>\n",
       "<td>0.0477431</td>\n",
       "<td>0.4026211</td>\n",
       "<td>0.3994369</td>\n",
       "<td>0.0118950</td>\n",
       "<td>0.9954881</td>\n",
       "<td>-88.0967108</td>\n",
       "<td>65.9201890</td>\n",
       "<td>0.5222327</td></tr>\n",
       "<tr><td>13</td>\n",
       "<td>0.7000100</td>\n",
       "<td>0.0137381</td>\n",
       "<td>0.0369045</td>\n",
       "<td>1.4273792</td>\n",
       "<td>0.0089552</td>\n",
       "<td>0.0213463</td>\n",
       "<td>0.3463671</td>\n",
       "<td>0.3454086</td>\n",
       "<td>0.0036916</td>\n",
       "<td>0.9991797</td>\n",
       "<td>-96.3095515</td>\n",
       "<td>42.7379212</td>\n",
       "<td>0.3950267</td></tr>\n",
       "<tr><td>14</td>\n",
       "<td>0.7999403</td>\n",
       "<td>0.0054742</td>\n",
       "<td>0.0082092</td>\n",
       "<td>1.2500933</td>\n",
       "<td>0.0019920</td>\n",
       "<td>0.0090671</td>\n",
       "<td>0.3033470</td>\n",
       "<td>0.3033921</td>\n",
       "<td>0.0008203</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.1790835</td>\n",
       "<td>25.0093318</td>\n",
       "<td>0.2641609</td></tr>\n",
       "<tr><td>15</td>\n",
       "<td>0.8999701</td>\n",
       "<td>0.0017304</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111480</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0033385</td>\n",
       "<td>0.2696306</td>\n",
       "<td>0.2700417</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1147976</td>\n",
       "<td>0.1320804</td></tr>\n",
       "<tr><td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0001166</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0009780</td>\n",
       "<td>0.2426595</td>\n",
       "<td>0.2431273</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
       "-------  --------------------------  -----------------  ----------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
       "1        0.0100528                   0.9975             4.121       4.121              1                0.998139     1                           0.998139            0.0414274       0.0414274                  312.1     312.1              0.0414274\n",
       "2        0.020006                    0.995519           4.121       4.121              1                0.996584     1                           0.997365            0.0410172       0.0824446                  312.1     312.1              0.0824446\n",
       "3        0.0300587                   0.992529           4.121       4.121              1                0.994137     1                           0.996286            0.0414274       0.123872                   312.1     312.1              0.123872\n",
       "4        0.0400119                   0.986537           4.121       4.121              1                0.990162     1                           0.994762            0.0410172       0.164889                   312.1     312.1              0.164889\n",
       "5        0.0500647                   0.975193           4.121       4.121              1                0.981037     1                           0.992006            0.0414274       0.206317                   312.1     312.1              0.206317\n",
       "6        0.10003                     0.786814           3.89935     4.01029            0.946215         0.88205      0.973134                    0.937083            0.194832        0.401148                   289.935   301.029            0.3976\n",
       "7        0.149995                    0.654847           3.17695     3.73269            0.770916         0.720109     0.905773                    0.864806            0.158737        0.559885                   217.695   273.269            0.541223\n",
       "8        0.20006                     0.520189           2.7446      3.48542            0.666004         0.583676     0.845771                    0.794454            0.137408        0.697293                   174.46    248.542            0.656552\n",
       "9        0.29999                     0.322405           1.68698     2.88634            0.409363         0.410929     0.700398                    0.666697            0.168581        0.865874                   68.6983   188.634            0.747198\n",
       "10       0.40002                     0.16224            0.852904    2.37786            0.206965         0.236906     0.577009                    0.559222            0.0853158       0.951189                   -14.7096  137.786            0.72777\n",
       "11       0.50005                     0.0708357          0.323939    1.96699            0.078607         0.111797     0.477309                    0.46972             0.0324036       0.983593                   -67.6061  96.699             0.638475\n",
       "12       0.59998                     0.0306384          0.119033    1.6592             0.0288845        0.0477431    0.402621                    0.399437            0.011895        0.995488                   -88.0967  65.9202            0.522233\n",
       "13       0.70001                     0.0137381          0.0369045   1.42738            0.00895522       0.0213463    0.346367                    0.345409            0.00369155      0.99918                    -96.3096  42.7379            0.395027\n",
       "14       0.79994                     0.00547419         0.00820916  1.25009            0.00199203       0.00906713   0.303347                    0.303392            0.000820345     1                          -99.1791  25.0093            0.264161\n",
       "15       0.89997                     0.00173041         0           1.11115            0                0.00333848   0.269631                    0.270042            0               1                          -100      11.1148            0.13208\n",
       "16       1                           0.000116623        0           1                  0                0.000978044  0.24266                     0.243127            0               1                          -100      0                  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.08901706244242294\n",
      "RMSE: 0.2983572731515405\n",
      "LogLoss: 0.2803344921150826\n",
      "Null degrees of freedom: 19679\n",
      "Residual degrees of freedom: 19673\n",
      "Null deviance: 21801.12781023197\n",
      "Residual deviance: 11033.965609649653\n",
      "AIC: 11047.965609649653\n",
      "AUC: 0.9275155885620863\n",
      "AUCPR: 0.8282390204996729\n",
      "Gini: 0.8550311771241725\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.37399972153697447: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b><=50K</b></td>\n",
       "<td><b>>50K</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td><=50K</td>\n",
       "<td>13297.0</td>\n",
       "<td>1612.0</td>\n",
       "<td>0.1081</td>\n",
       "<td> (1612.0/14909.0)</td></tr>\n",
       "<tr><td>>50K</td>\n",
       "<td>1104.0</td>\n",
       "<td>3667.0</td>\n",
       "<td>0.2314</td>\n",
       "<td> (1104.0/4771.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>14401.0</td>\n",
       "<td>5279.0</td>\n",
       "<td>0.138</td>\n",
       "<td> (2716.0/19680.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       <=50K    >50K    Error    Rate\n",
       "-----  -------  ------  -------  ----------------\n",
       "<=50K  13297    1612    0.1081   (1612.0/14909.0)\n",
       ">50K   1104     3667    0.2314   (1104.0/4771.0)\n",
       "Total  14401    5279    0.138    (2716.0/19680.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3739997</td>\n",
       "<td>0.7297512</td>\n",
       "<td>211.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1484512</td>\n",
       "<td>0.8111916</td>\n",
       "<td>303.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6492051</td>\n",
       "<td>0.7655269</td>\n",
       "<td>118.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5068508</td>\n",
       "<td>0.8718496</td>\n",
       "<td>163.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9981910</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0010301</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9981910</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4183810</td>\n",
       "<td>0.6423844</td>\n",
       "<td>194.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2818292</td>\n",
       "<td>0.8434291</td>\n",
       "<td>246.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.2364105</td>\n",
       "<td>0.8450239</td>\n",
       "<td>264.0</td></tr>\n",
       "<tr><td>max tns</td>\n",
       "<td>0.9981910</td>\n",
       "<td>14909.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fns</td>\n",
       "<td>0.9981910</td>\n",
       "<td>4674.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fps</td>\n",
       "<td>0.0006774</td>\n",
       "<td>14909.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tps</td>\n",
       "<td>0.0010301</td>\n",
       "<td>4771.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max tnr</td>\n",
       "<td>0.9981910</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fnr</td>\n",
       "<td>0.9981910</td>\n",
       "<td>0.9796688</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fpr</td>\n",
       "<td>0.0006774</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tpr</td>\n",
       "<td>0.0010301</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.374        0.729751  211\n",
       "max f2                       0.148451     0.811192  303\n",
       "max f0point5                 0.649205     0.765527  118\n",
       "max accuracy                 0.506851     0.87185   163\n",
       "max precision                0.998191     1         0\n",
       "max recall                   0.00103014   1         398\n",
       "max specificity              0.998191     1         0\n",
       "max absolute_mcc             0.418381     0.642384  194\n",
       "max min_per_class_accuracy   0.281829     0.843429  246\n",
       "max mean_per_class_accuracy  0.236411     0.845024  264\n",
       "max tns                      0.998191     14909     0\n",
       "max fns                      0.998191     4674      0\n",
       "max fps                      0.000677397  14909     399\n",
       "max tps                      0.00103014   4771      398\n",
       "max tnr                      0.998191     1         0\n",
       "max fnr                      0.998191     0.979669  0\n",
       "max fpr                      0.000677397  1         399\n",
       "max tpr                      0.00103014   1         398"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 24.24 %, avg score: 24.25 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td>\n",
       "<td><b>kolmogorov_smirnov</b></td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0100102</td>\n",
       "<td>0.9966790</td>\n",
       "<td>4.1249214</td>\n",
       "<td>4.1249214</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9977215</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9977215</td>\n",
       "<td>0.0412911</td>\n",
       "<td>0.0412911</td>\n",
       "<td>312.4921400</td>\n",
       "<td>312.4921400</td>\n",
       "<td>0.0412911</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.0200203</td>\n",
       "<td>0.9942997</td>\n",
       "<td>4.1249214</td>\n",
       "<td>4.1249214</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9954928</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9966071</td>\n",
       "<td>0.0412911</td>\n",
       "<td>0.0825823</td>\n",
       "<td>312.4921400</td>\n",
       "<td>312.4921400</td>\n",
       "<td>0.0825823</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>0.0300305</td>\n",
       "<td>0.9909762</td>\n",
       "<td>4.0830440</td>\n",
       "<td>4.1109623</td>\n",
       "<td>0.9898477</td>\n",
       "<td>0.9928464</td>\n",
       "<td>0.9966159</td>\n",
       "<td>0.9953536</td>\n",
       "<td>0.0408719</td>\n",
       "<td>0.1234542</td>\n",
       "<td>308.3044026</td>\n",
       "<td>311.0962275</td>\n",
       "<td>0.1233201</td></tr>\n",
       "<tr><td>4</td>\n",
       "<td>0.0400407</td>\n",
       "<td>0.9826635</td>\n",
       "<td>4.1039827</td>\n",
       "<td>4.1092174</td>\n",
       "<td>0.9949239</td>\n",
       "<td>0.9873531</td>\n",
       "<td>0.9961929</td>\n",
       "<td>0.9933535</td>\n",
       "<td>0.0410815</td>\n",
       "<td>0.1645357</td>\n",
       "<td>310.3982713</td>\n",
       "<td>310.9217385</td>\n",
       "<td>0.1643345</td></tr>\n",
       "<tr><td>5</td>\n",
       "<td>0.05</td>\n",
       "<td>0.9680361</td>\n",
       "<td>4.1038759</td>\n",
       "<td>4.1081534</td>\n",
       "<td>0.9948980</td>\n",
       "<td>0.9760964</td>\n",
       "<td>0.9959350</td>\n",
       "<td>0.9899161</td>\n",
       "<td>0.0408719</td>\n",
       "<td>0.2054077</td>\n",
       "<td>310.3875883</td>\n",
       "<td>310.8153427</td>\n",
       "<td>0.2051394</td></tr>\n",
       "<tr><td>6</td>\n",
       "<td>0.1</td>\n",
       "<td>0.7752008</td>\n",
       "<td>3.5925383</td>\n",
       "<td>3.8503458</td>\n",
       "<td>0.8709350</td>\n",
       "<td>0.8652481</td>\n",
       "<td>0.9334350</td>\n",
       "<td>0.9275821</td>\n",
       "<td>0.1796269</td>\n",
       "<td>0.3850346</td>\n",
       "<td>259.2538252</td>\n",
       "<td>285.0345839</td>\n",
       "<td>0.3762479</td></tr>\n",
       "<tr><td>7</td>\n",
       "<td>0.15</td>\n",
       "<td>0.6401044</td>\n",
       "<td>2.9302033</td>\n",
       "<td>3.5436317</td>\n",
       "<td>0.7103659</td>\n",
       "<td>0.7091983</td>\n",
       "<td>0.8590786</td>\n",
       "<td>0.8547875</td>\n",
       "<td>0.1465102</td>\n",
       "<td>0.5315447</td>\n",
       "<td>193.0203312</td>\n",
       "<td>254.3631664</td>\n",
       "<td>0.5036421</td></tr>\n",
       "<tr><td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.5108817</td>\n",
       "<td>2.3349403</td>\n",
       "<td>3.2414588</td>\n",
       "<td>0.5660569</td>\n",
       "<td>0.5750862</td>\n",
       "<td>0.7858232</td>\n",
       "<td>0.7848622</td>\n",
       "<td>0.1167470</td>\n",
       "<td>0.6482918</td>\n",
       "<td>133.4940264</td>\n",
       "<td>224.1458814</td>\n",
       "<td>0.5917487</td></tr>\n",
       "<tr><td>9</td>\n",
       "<td>0.3</td>\n",
       "<td>0.3186673</td>\n",
       "<td>1.6202054</td>\n",
       "<td>2.7010410</td>\n",
       "<td>0.3927846</td>\n",
       "<td>0.4087394</td>\n",
       "<td>0.6548103</td>\n",
       "<td>0.6594879</td>\n",
       "<td>0.1620205</td>\n",
       "<td>0.8103123</td>\n",
       "<td>62.0205408</td>\n",
       "<td>170.1041012</td>\n",
       "<td>0.6736163</td></tr>\n",
       "<tr><td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1688170</td>\n",
       "<td>1.0333263</td>\n",
       "<td>2.2841123</td>\n",
       "<td>0.2505081</td>\n",
       "<td>0.2394505</td>\n",
       "<td>0.5537348</td>\n",
       "<td>0.5544786</td>\n",
       "<td>0.1033326</td>\n",
       "<td>0.9136449</td>\n",
       "<td>3.3326347</td>\n",
       "<td>128.4112345</td>\n",
       "<td>0.6780155</td></tr>\n",
       "<tr><td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0752723</td>\n",
       "<td>0.5177112</td>\n",
       "<td>1.9308321</td>\n",
       "<td>0.1255081</td>\n",
       "<td>0.1179900</td>\n",
       "<td>0.4680894</td>\n",
       "<td>0.4671808</td>\n",
       "<td>0.0517711</td>\n",
       "<td>0.9654161</td>\n",
       "<td>-48.2288828</td>\n",
       "<td>93.0832111</td>\n",
       "<td>0.6143529</td></tr>\n",
       "<tr><td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.0323615</td>\n",
       "<td>0.2054077</td>\n",
       "<td>1.6432614</td>\n",
       "<td>0.0497967</td>\n",
       "<td>0.0506327</td>\n",
       "<td>0.3983740</td>\n",
       "<td>0.3977562</td>\n",
       "<td>0.0205408</td>\n",
       "<td>0.9859568</td>\n",
       "<td>-79.4592329</td>\n",
       "<td>64.3261371</td>\n",
       "<td>0.5094661</td></tr>\n",
       "<tr><td>13</td>\n",
       "<td>0.7</td>\n",
       "<td>0.0150451</td>\n",
       "<td>0.0880319</td>\n",
       "<td>1.4210857</td>\n",
       "<td>0.0213415</td>\n",
       "<td>0.0227005</td>\n",
       "<td>0.3445122</td>\n",
       "<td>0.3441768</td>\n",
       "<td>0.0088032</td>\n",
       "<td>0.9947600</td>\n",
       "<td>-91.1968141</td>\n",
       "<td>42.1085726</td>\n",
       "<td>0.3890856</td></tr>\n",
       "<tr><td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0065861</td>\n",
       "<td>0.0377279</td>\n",
       "<td>1.2481660</td>\n",
       "<td>0.0091463</td>\n",
       "<td>0.0104275</td>\n",
       "<td>0.3025915</td>\n",
       "<td>0.3024581</td>\n",
       "<td>0.0037728</td>\n",
       "<td>0.9985328</td>\n",
       "<td>-96.2272060</td>\n",
       "<td>24.8166003</td>\n",
       "<td>0.2620649</td></tr>\n",
       "<tr><td>15</td>\n",
       "<td>0.9</td>\n",
       "<td>0.0019346</td>\n",
       "<td>0.0104800</td>\n",
       "<td>1.1106453</td>\n",
       "<td>0.0025407</td>\n",
       "<td>0.0039926</td>\n",
       "<td>0.2692525</td>\n",
       "<td>0.2692953</td>\n",
       "<td>0.0010480</td>\n",
       "<td>0.9995808</td>\n",
       "<td>-98.9520017</td>\n",
       "<td>11.0645334</td>\n",
       "<td>0.1314475</td></tr>\n",
       "<tr><td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0001637</td>\n",
       "<td>0.0041920</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0010163</td>\n",
       "<td>0.0010517</td>\n",
       "<td>0.2424289</td>\n",
       "<td>0.2424709</td>\n",
       "<td>0.0004192</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.5808007</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score       cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
       "-------  --------------------------  -----------------  ----------  -----------------  ---------------  ----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
       "1        0.0100102                   0.996679           4.12492     4.12492            1                0.997721    1                           0.997721            0.0412911       0.0412911                  312.492   312.492            0.0412911\n",
       "2        0.0200203                   0.9943             4.12492     4.12492            1                0.995493    1                           0.996607            0.0412911       0.0825823                  312.492   312.492            0.0825823\n",
       "3        0.0300305                   0.990976           4.08304     4.11096            0.989848         0.992846    0.996616                    0.995354            0.0408719       0.123454                   308.304   311.096            0.12332\n",
       "4        0.0400407                   0.982664           4.10398     4.10922            0.994924         0.987353    0.996193                    0.993353            0.0410815       0.164536                   310.398   310.922            0.164335\n",
       "5        0.05                        0.968036           4.10388     4.10815            0.994898         0.976096    0.995935                    0.989916            0.0408719       0.205408                   310.388   310.815            0.205139\n",
       "6        0.1                         0.775201           3.59254     3.85035            0.870935         0.865248    0.933435                    0.927582            0.179627        0.385035                   259.254   285.035            0.376248\n",
       "7        0.15                        0.640104           2.9302      3.54363            0.710366         0.709198    0.859079                    0.854787            0.14651         0.531545                   193.02    254.363            0.503642\n",
       "8        0.2                         0.510882           2.33494     3.24146            0.566057         0.575086    0.785823                    0.784862            0.116747        0.648292                   133.494   224.146            0.591749\n",
       "9        0.3                         0.318667           1.62021     2.70104            0.392785         0.408739    0.65481                     0.659488            0.162021        0.810312                   62.0205   170.104            0.673616\n",
       "10       0.4                         0.168817           1.03333     2.28411            0.250508         0.239451    0.553735                    0.554479            0.103333        0.913645                   3.33263   128.411            0.678015\n",
       "11       0.5                         0.0752723          0.517711    1.93083            0.125508         0.11799     0.468089                    0.467181            0.0517711       0.965416                   -48.2289  93.0832            0.614353\n",
       "12       0.6                         0.0323615          0.205408    1.64326            0.0497967        0.0506327   0.398374                    0.397756            0.0205408       0.985957                   -79.4592  64.3261            0.509466\n",
       "13       0.7                         0.0150451          0.0880319   1.42109            0.0213415        0.0227005   0.344512                    0.344177            0.00880319      0.99476                    -91.1968  42.1086            0.389086\n",
       "14       0.8                         0.00658614         0.0377279   1.24817            0.00914634       0.0104275   0.302591                    0.302458            0.00377279      0.998533                   -96.2272  24.8166            0.262065\n",
       "15       0.9                         0.00193465         0.01048     1.11065            0.00254065       0.00399264  0.269252                    0.269295            0.001048        0.999581                   -98.952   11.0645            0.131447\n",
       "16       1                           0.000163736        0.00419199  1                  0.00101626       0.0010517   0.242429                    0.242471            0.000419199     1                          -99.5808  0                  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## Model performance on testing set\n",
    "The measure of a model's likely performance in production is its performance on unseen data. Therefore it is common to hold out unseen a portion of the data as a testing set, and the model's performance measured against its ground truth.\n",
    "\n",
    "We can do this here by showing the model running predictions on the testing data (class probabilities), and analyzing its performance via the `model_performance()` method. This shows similar information to the `leader()` method above. We see that the model generalizes quite well to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "model = aml.leader\n",
    "predictions = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>predict  </th><th style=\"text-align: right;\">    &lt;=50K</th><th style=\"text-align: right;\">      &gt;50K</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>&lt;=50K    </td><td style=\"text-align: right;\">0.909669 </td><td style=\"text-align: right;\">0.090331  </td></tr>\n",
       "<tr><td>&gt;50K     </td><td style=\"text-align: right;\">0.538777 </td><td style=\"text-align: right;\">0.461223  </td></tr>\n",
       "<tr><td>&gt;50K     </td><td style=\"text-align: right;\">0.0122868</td><td style=\"text-align: right;\">0.987713  </td></tr>\n",
       "<tr><td>&lt;=50K    </td><td style=\"text-align: right;\">0.997832 </td><td style=\"text-align: right;\">0.00216811</td></tr>\n",
       "<tr><td>&lt;=50K    </td><td style=\"text-align: right;\">0.976717 </td><td style=\"text-align: right;\">0.0232834 </td></tr>\n",
       "<tr><td>&lt;=50K    </td><td style=\"text-align: right;\">0.939607 </td><td style=\"text-align: right;\">0.0603929 </td></tr>\n",
       "<tr><td>&lt;=50K    </td><td style=\"text-align: right;\">0.715487 </td><td style=\"text-align: right;\">0.284513  </td></tr>\n",
       "<tr><td>&lt;=50K    </td><td style=\"text-align: right;\">0.998162 </td><td style=\"text-align: right;\">0.00183815</td></tr>\n",
       "<tr><td>&lt;=50K    </td><td style=\"text-align: right;\">0.711692 </td><td style=\"text-align: right;\">0.288308  </td></tr>\n",
       "<tr><td>&lt;=50K    </td><td style=\"text-align: right;\">0.995555 </td><td style=\"text-align: right;\">0.00444468</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.08495665870546716\n",
      "RMSE: 0.29147325555780784\n",
      "LogLoss: 0.26725783945281806\n",
      "Null degrees of freedom: 6384\n",
      "Residual degrees of freedom: 6378\n",
      "Null deviance: 6997.844809115317\n",
      "Residual deviance: 3412.8826098124864\n",
      "AIC: 3426.8826098124864\n",
      "AUC: 0.9337852820189617\n",
      "AUCPR: 0.8371945381466177\n",
      "Gini: 0.8675705640379234\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.39554547194115: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b><=50K</b></td>\n",
       "<td><b>>50K</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td><=50K</td>\n",
       "<td>4410.0</td>\n",
       "<td>460.0</td>\n",
       "<td>0.0945</td>\n",
       "<td> (460.0/4870.0)</td></tr>\n",
       "<tr><td>>50K</td>\n",
       "<td>350.0</td>\n",
       "<td>1165.0</td>\n",
       "<td>0.231</td>\n",
       "<td> (350.0/1515.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>4760.0</td>\n",
       "<td>1625.0</td>\n",
       "<td>0.1269</td>\n",
       "<td> (810.0/6385.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       <=50K    >50K    Error    Rate\n",
       "-----  -------  ------  -------  --------------\n",
       "<=50K  4410     460     0.0945   (460.0/4870.0)\n",
       ">50K   350      1165    0.231    (350.0/1515.0)\n",
       "Total  4760     1625    0.1269   (810.0/6385.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3955455</td>\n",
       "<td>0.7420382</td>\n",
       "<td>198.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2023667</td>\n",
       "<td>0.8112445</td>\n",
       "<td>275.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6903900</td>\n",
       "<td>0.7737241</td>\n",
       "<td>98.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4918037</td>\n",
       "<td>0.8815975</td>\n",
       "<td>163.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9984129</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0084844</td>\n",
       "<td>1.0</td>\n",
       "<td>386.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9984129</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4748854</td>\n",
       "<td>0.6613660</td>\n",
       "<td>169.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2891045</td>\n",
       "<td>0.8475248</td>\n",
       "<td>239.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.2023667</td>\n",
       "<td>0.8490577</td>\n",
       "<td>275.0</td></tr>\n",
       "<tr><td>max tns</td>\n",
       "<td>0.9984129</td>\n",
       "<td>4870.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fns</td>\n",
       "<td>0.9984129</td>\n",
       "<td>1479.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fps</td>\n",
       "<td>0.0005955</td>\n",
       "<td>4870.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tps</td>\n",
       "<td>0.0084844</td>\n",
       "<td>1515.0</td>\n",
       "<td>386.0</td></tr>\n",
       "<tr><td>max tnr</td>\n",
       "<td>0.9984129</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fnr</td>\n",
       "<td>0.9984129</td>\n",
       "<td>0.9762376</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fpr</td>\n",
       "<td>0.0005955</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tpr</td>\n",
       "<td>0.0084844</td>\n",
       "<td>1.0</td>\n",
       "<td>386.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.395545     0.742038  198\n",
       "max f2                       0.202367     0.811245  275\n",
       "max f0point5                 0.69039      0.773724  98\n",
       "max accuracy                 0.491804     0.881597  163\n",
       "max precision                0.998413     1         0\n",
       "max recall                   0.00848436   1         386\n",
       "max specificity              0.998413     1         0\n",
       "max absolute_mcc             0.474885     0.661366  169\n",
       "max min_per_class_accuracy   0.289104     0.847525  239\n",
       "max mean_per_class_accuracy  0.202367     0.849058  275\n",
       "max tns                      0.998413     4870      0\n",
       "max fns                      0.998413     1479      0\n",
       "max fps                      0.000595467  4870      399\n",
       "max tps                      0.00848436   1515      386\n",
       "max tnr                      0.998413     1         0\n",
       "max fnr                      0.998413     0.976238  0\n",
       "max fpr                      0.000595467  1         399\n",
       "max tpr                      0.00848436   1         386"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 23.73 %, avg score: 24.08 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td>\n",
       "<td><b>kolmogorov_smirnov</b></td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.0100235</td>\n",
       "<td>0.9971625</td>\n",
       "<td>4.2145215</td>\n",
       "<td>4.2145215</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9980595</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9980595</td>\n",
       "<td>0.0422442</td>\n",
       "<td>0.0422442</td>\n",
       "<td>321.4521452</td>\n",
       "<td>321.4521452</td>\n",
       "<td>0.0422442</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.0200470</td>\n",
       "<td>0.9946995</td>\n",
       "<td>4.2145215</td>\n",
       "<td>4.2145215</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9959285</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9969940</td>\n",
       "<td>0.0422442</td>\n",
       "<td>0.0844884</td>\n",
       "<td>321.4521452</td>\n",
       "<td>321.4521452</td>\n",
       "<td>0.0844884</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>0.0300705</td>\n",
       "<td>0.9913300</td>\n",
       "<td>4.2145215</td>\n",
       "<td>4.2145215</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9932932</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9957604</td>\n",
       "<td>0.0422442</td>\n",
       "<td>0.1267327</td>\n",
       "<td>321.4521452</td>\n",
       "<td>321.4521452</td>\n",
       "<td>0.1267327</td></tr>\n",
       "<tr><td>4</td>\n",
       "<td>0.0400940</td>\n",
       "<td>0.9831519</td>\n",
       "<td>4.2145215</td>\n",
       "<td>4.2145215</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9879531</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9938086</td>\n",
       "<td>0.0422442</td>\n",
       "<td>0.1689769</td>\n",
       "<td>321.4521452</td>\n",
       "<td>321.4521452</td>\n",
       "<td>0.1689769</td></tr>\n",
       "<tr><td>5</td>\n",
       "<td>0.0501175</td>\n",
       "<td>0.9646815</td>\n",
       "<td>4.2145215</td>\n",
       "<td>4.2145215</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9751623</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9900793</td>\n",
       "<td>0.0422442</td>\n",
       "<td>0.2112211</td>\n",
       "<td>321.4521452</td>\n",
       "<td>321.4521452</td>\n",
       "<td>0.2112211</td></tr>\n",
       "<tr><td>6</td>\n",
       "<td>0.1000783</td>\n",
       "<td>0.7748916</td>\n",
       "<td>3.6860548</td>\n",
       "<td>3.9507016</td>\n",
       "<td>0.8746082</td>\n",
       "<td>0.8631609</td>\n",
       "<td>0.9374022</td>\n",
       "<td>0.9267194</td>\n",
       "<td>0.1841584</td>\n",
       "<td>0.3953795</td>\n",
       "<td>268.6054812</td>\n",
       "<td>295.0701643</td>\n",
       "<td>0.3871660</td></tr>\n",
       "<tr><td>7</td>\n",
       "<td>0.1500392</td>\n",
       "<td>0.6425027</td>\n",
       "<td>2.9594132</td>\n",
       "<td>3.6206171</td>\n",
       "<td>0.7021944</td>\n",
       "<td>0.7073270</td>\n",
       "<td>0.8590814</td>\n",
       "<td>0.8536649</td>\n",
       "<td>0.1478548</td>\n",
       "<td>0.5432343</td>\n",
       "<td>195.9413183</td>\n",
       "<td>262.0617072</td>\n",
       "<td>0.5155136</td></tr>\n",
       "<tr><td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.5069073</td>\n",
       "<td>2.5234282</td>\n",
       "<td>3.3465347</td>\n",
       "<td>0.5987461</td>\n",
       "<td>0.5746836</td>\n",
       "<td>0.7940486</td>\n",
       "<td>0.7839742</td>\n",
       "<td>0.1260726</td>\n",
       "<td>0.6693069</td>\n",
       "<td>152.3428205</td>\n",
       "<td>234.6534653</td>\n",
       "<td>0.6153028</td></tr>\n",
       "<tr><td>9</td>\n",
       "<td>0.3000783</td>\n",
       "<td>0.3145347</td>\n",
       "<td>1.6027053</td>\n",
       "<td>2.7649548</td>\n",
       "<td>0.3802817</td>\n",
       "<td>0.4050825</td>\n",
       "<td>0.6560543</td>\n",
       "<td>0.6576111</td>\n",
       "<td>0.1603960</td>\n",
       "<td>0.8297030</td>\n",
       "<td>60.2705341</td>\n",
       "<td>176.4954836</td>\n",
       "<td>0.6943847</td></tr>\n",
       "<tr><td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1639263</td>\n",
       "<td>0.8917875</td>\n",
       "<td>2.2970297</td>\n",
       "<td>0.2115987</td>\n",
       "<td>0.2331320</td>\n",
       "<td>0.5450274</td>\n",
       "<td>0.5515744</td>\n",
       "<td>0.0891089</td>\n",
       "<td>0.9188119</td>\n",
       "<td>-10.8212545</td>\n",
       "<td>129.7029703</td>\n",
       "<td>0.6802082</td></tr>\n",
       "<tr><td>11</td>\n",
       "<td>0.5000783</td>\n",
       "<td>0.0761372</td>\n",
       "<td>0.5540216</td>\n",
       "<td>1.9482097</td>\n",
       "<td>0.1314554</td>\n",
       "<td>0.1150369</td>\n",
       "<td>0.4622612</td>\n",
       "<td>0.4642122</td>\n",
       "<td>0.0554455</td>\n",
       "<td>0.9742574</td>\n",
       "<td>-44.5978401</td>\n",
       "<td>94.8209729</td>\n",
       "<td>0.6216907</td></tr>\n",
       "<tr><td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.0323125</td>\n",
       "<td>0.1585400</td>\n",
       "<td>1.6501650</td>\n",
       "<td>0.0376176</td>\n",
       "<td>0.0509393</td>\n",
       "<td>0.3915427</td>\n",
       "<td>0.3953873</td>\n",
       "<td>0.0158416</td>\n",
       "<td>0.9900990</td>\n",
       "<td>-84.1460008</td>\n",
       "<td>65.0165017</td>\n",
       "<td>0.5114542</td></tr>\n",
       "<tr><td>13</td>\n",
       "<td>0.6999217</td>\n",
       "<td>0.0141783</td>\n",
       "<td>0.0858758</td>\n",
       "<td>1.4268451</td>\n",
       "<td>0.0203762</td>\n",
       "<td>0.0221104</td>\n",
       "<td>0.3385545</td>\n",
       "<td>0.3420978</td>\n",
       "<td>0.0085809</td>\n",
       "<td>0.9986799</td>\n",
       "<td>-91.4124171</td>\n",
       "<td>42.6845146</td>\n",
       "<td>0.3916983</td></tr>\n",
       "<tr><td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0056595</td>\n",
       "<td>0.0131910</td>\n",
       "<td>1.25</td>\n",
       "<td>0.0031299</td>\n",
       "<td>0.0094720</td>\n",
       "<td>0.2965936</td>\n",
       "<td>0.3004871</td>\n",
       "<td>0.0013201</td>\n",
       "<td>1.0</td>\n",
       "<td>-98.6809010</td>\n",
       "<td>25.0</td>\n",
       "<td>0.2622177</td></tr>\n",
       "<tr><td>15</td>\n",
       "<td>0.8999217</td>\n",
       "<td>0.0017605</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1112078</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0034346</td>\n",
       "<td>0.2636617</td>\n",
       "<td>0.2675042</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1207797</td>\n",
       "<td>0.1312115</td></tr>\n",
       "<tr><td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0001775</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0009635</td>\n",
       "<td>0.2372749</td>\n",
       "<td>0.2408293</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
       "-------  --------------------------  -----------------  ---------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
       "1        0.0100235                   0.997163           4.21452    4.21452            1                0.99806      1                           0.99806             0.0422442       0.0422442                  321.452   321.452            0.0422442\n",
       "2        0.020047                    0.9947             4.21452    4.21452            1                0.995928     1                           0.996994            0.0422442       0.0844884                  321.452   321.452            0.0844884\n",
       "3        0.0300705                   0.99133            4.21452    4.21452            1                0.993293     1                           0.99576             0.0422442       0.126733                   321.452   321.452            0.126733\n",
       "4        0.040094                    0.983152           4.21452    4.21452            1                0.987953     1                           0.993809            0.0422442       0.168977                   321.452   321.452            0.168977\n",
       "5        0.0501175                   0.964681           4.21452    4.21452            1                0.975162     1                           0.990079            0.0422442       0.211221                   321.452   321.452            0.211221\n",
       "6        0.100078                    0.774892           3.68605    3.9507             0.874608         0.863161     0.937402                    0.926719            0.184158        0.39538                    268.605   295.07             0.387166\n",
       "7        0.150039                    0.642503           2.95941    3.62062            0.702194         0.707327     0.859081                    0.853665            0.147855        0.543234                   195.941   262.062            0.515514\n",
       "8        0.2                         0.506907           2.52343    3.34653            0.598746         0.574684     0.794049                    0.783974            0.126073        0.669307                   152.343   234.653            0.615303\n",
       "9        0.300078                    0.314535           1.60271    2.76495            0.380282         0.405083     0.656054                    0.657611            0.160396        0.829703                   60.2705   176.495            0.694385\n",
       "10       0.4                         0.163926           0.891787   2.29703            0.211599         0.233132     0.545027                    0.551574            0.0891089       0.918812                   -10.8213  129.703            0.680208\n",
       "11       0.500078                    0.0761372          0.554022   1.94821            0.131455         0.115037     0.462261                    0.464212            0.0554455       0.974257                   -44.5978  94.821             0.621691\n",
       "12       0.6                         0.0323125          0.15854    1.65017            0.0376176        0.0509393    0.391543                    0.395387            0.0158416       0.990099                   -84.146   65.0165            0.511454\n",
       "13       0.699922                    0.0141783          0.0858758  1.42685            0.0203762        0.0221104    0.338554                    0.342098            0.00858086      0.99868                    -91.4124  42.6845            0.391698\n",
       "14       0.8                         0.00565949         0.013191   1.25               0.00312989       0.00947198   0.296594                    0.300487            0.00132013      1                          -98.6809  25                 0.262218\n",
       "15       0.899922                    0.00176053         0          1.11121            0                0.00343465   0.263662                    0.267504            0               1                          -100      11.1208            0.131211\n",
       "16       1                           0.000177497        0          1                  0                0.000963544  0.237275                    0.240829            0               1                          -100      0                  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_performance(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## Save the model for deployment\n",
    "\n",
    "Finally, for a model to be put into production, it needs to be saved in a manner that can be accessed later. H2O has several model formats, but the one most [preferred for production](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html) is MOJO, or modified Java object. This allows the most general functionality and datatypes to be passed.\n",
    "\n",
    "The model is output as a .zip file that includes its single Java dependency, `h2o-genmodel.jar`. Java knowledge is therefore required to proceed to production deployment, but the format allows significant flexibility in where it can be deployed.\n",
    "\n",
    "The location that we save the model to is the Gradient-provided storage corresponding to this notebook, at `/storage`.\n",
    "\n",
    "In the command line section of this project (refer back to https://github.com/gradient-ai/Gradient-Boosted-Trees-and-AutoML), we will deploy this model on Gradient as a REST endpoint, and send inference data to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": true,
     "source_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /storage/StackedEnsemble_AllModels_AutoML_20210721_040517.zip\n"
     ]
    }
   ],
   "source": [
    "modelfile = model.download_mojo(path=\"/storage\", get_genmodel_jar=True)\n",
    "print(\"Model saved to \" + modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "We have shown\n",
    "\n",
    " - Setup Java and H2O on Gradient\n",
    " - Load and prepare small dataset (UCI Census Income)\n",
    " - Train gradient-boosted decision tree and other models using H2O's AutoML\n",
    " - Evaluate model performance on unseen testing data\n",
    " - Save model so that it can be deployed to production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "## Next Steps\n",
    "To see the Workflow portion of this project, or to deploy the model using the command line, refer back to the project GitHub repo at https://github.com/gradient-ai/Gradient-Boosted-Trees-and-AutoML ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
